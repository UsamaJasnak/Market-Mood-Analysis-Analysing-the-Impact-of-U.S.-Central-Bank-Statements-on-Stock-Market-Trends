{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset, ClassLabel, Features, Value\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from huggingface_hub import login\n",
    "from scipy.special import softmax\n",
    "\n",
    "from scripts import preprocessing as pr\n",
    "from scripts import utils\n",
    "from scripts import lstm_model as lm\n",
    "from scripts import gru_model as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Parquet file using PyArrow\n",
    "with open('.\\\\data\\\\output_speech_us_central_bank_v2.parquet', 'rb') as handle:\n",
    "    text = pq.read_table(handle).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = pr.TextPreprocessor(\n",
    "    remove_stopwords=False,\n",
    "    apply_pos_tagging=True,      # Enable POS tagging\n",
    "    apply_lemmatization=False     # Enable lemmatization\n",
    ")\n",
    "\n",
    "text[['speech_text_processed_text', 'speech_text_word_tokens',\n",
    "      'speech_text_sent_tokens', 'speech_text_word_tokens_wo_stopwords',\n",
    "      'speech_text_pos_tags']] = preprocessor.preprocess_dataframe(text, 'speech_text')\n",
    "\n",
    "text.dropna(inplace=True)\n",
    "\n",
    "text['processed_speech_text'] = text['speech_text_word_tokens'].apply(' '.join)  # speech_text_word_tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED USAMA\\AppData\\Local\\Temp\\ipykernel_25548\\1968621960.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sentiment_encoded'] = label_encoder.fit_transform(data['target_label'])\n"
     ]
    }
   ],
   "source": [
    "data = text[['speech_text_processed_text', 'target_label']]\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(data['speech_text_processed_text'])\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the sentiment labels\n",
    "data['sentiment_encoded'] = label_encoder.fit_transform(data['target_label'])\n",
    "y = data['sentiment_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, data.index, \n",
    "                                                                                 stratify=y, test_size=0.2, \n",
    "                                                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.875862</td>\n",
       "      <td>0.734234</td>\n",
       "      <td>0.671697</td>\n",
       "      <td>0.776539</td>\n",
       "      <td>0.71131</td>\n",
       "      <td>0.819775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.755952</td>\n",
       "      <td>0.734234</td>\n",
       "      <td>0.711310</td>\n",
       "      <td>0.734234</td>\n",
       "      <td>0.71131</td>\n",
       "      <td>0.819775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.549618</td>\n",
       "      <td>0.811502</td>\n",
       "      <td>0.734234</td>\n",
       "      <td>0.680560</td>\n",
       "      <td>0.747800</td>\n",
       "      <td>0.71131</td>\n",
       "      <td>0.819775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.734234</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>0.71131</td>\n",
       "      <td>0.819775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1  accuracy   macro avg  weighted avg  \\\n",
       "precision   0.467532    0.875862  0.734234    0.671697      0.776539   \n",
       "recall      0.666667    0.755952  0.734234    0.711310      0.734234   \n",
       "f1-score    0.549618    0.811502  0.734234    0.680560      0.747800   \n",
       "support    54.000000  168.000000  0.734234  222.000000    222.000000   \n",
       "\n",
       "           balanced accuracy       auc  \n",
       "precision            0.71131  0.819775  \n",
       "recall               0.71131  0.819775  \n",
       "f1-score             0.71131  0.819775  \n",
       "support              0.71131  0.819775  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 36  18]\n",
      " [ 41 127]]\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model\n",
    "model_lr = LogisticRegression(class_weight='balanced')\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "pred_prob_lr = model_lr.predict_proba(X_test)[:, 1]\n",
    "pred_label_lr = (pred_prob_lr >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "display(utils.test_report(Y_test=y_test, labels=pred_label_lr, probs=pred_prob_lr))\n",
    "print(confusion_matrix(y_test, pred_label_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rule Based using VADER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\MOHAMMED\n",
      "[nltk_data]     USAMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.657658</td>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.565256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.565256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.846561</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.483886</td>\n",
       "      <td>0.670125</td>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.565256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.565256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1  accuracy   macro avg  weighted avg  \\\n",
       "precision   0.333333    0.761905  0.738739    0.547619      0.657658   \n",
       "recall      0.074074    0.952381  0.738739    0.513228      0.738739   \n",
       "f1-score    0.121212    0.846561  0.738739    0.483886      0.670125   \n",
       "support    54.000000  168.000000  0.738739  222.000000    222.000000   \n",
       "\n",
       "           balanced accuracy       auc  \n",
       "precision           0.513228  0.565256  \n",
       "recall              0.513228  0.565256  \n",
       "f1-score            0.513228  0.565256  \n",
       "support             0.513228  0.565256  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4  50]\n",
      " [  8 160]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download VADER Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "## Setting up function for prediction\n",
    "def rule_based_sentiment(text):\n",
    "    score = sia.polarity_scores(text)\n",
    "    compound = score['compound']\n",
    "    if compound > 0:\n",
    "        return 1, compound  # Positive\n",
    "    else:\n",
    "        return 0, compound  # Negative\n",
    "\n",
    "# Storing test set using test indices\n",
    "vader_test_X = data.loc[indices_test, 'speech_text_processed_text']\n",
    "vader_test_y = data.loc[indices_test, 'sentiment_encoded']\n",
    "\n",
    "# Predicting on the test set\n",
    "pred_vader = vader_test_X.apply(rule_based_sentiment)\n",
    "\n",
    "pred_label_vader = pd.DataFrame(pred_vader.tolist())[0]\n",
    "pred_prob_vader = pd.DataFrame(pred_vader.tolist())[1]\n",
    "\n",
    "# Since vader model provides a score between -1 and 1, we will rescale it for a range between 0 and 1\n",
    "pred_prob_vader = (pred_prob_vader + 1) / 2\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "display(utils.test_report(Y_test=vader_test_y, labels=pred_label_vader, probs=pred_prob_vader))\n",
    "print(confusion_matrix(vader_test_y, pred_label_vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "1    210\n",
       "0     12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_vader.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.572681</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.613095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.613095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.651975</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.613095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>54.0</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.613095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1  accuracy   macro avg  weighted avg  \\\n",
       "precision   0.0    0.756757  0.756757    0.378378      0.572681   \n",
       "recall      0.0    1.000000  0.756757    0.500000      0.756757   \n",
       "f1-score    0.0    0.861538  0.756757    0.430769      0.651975   \n",
       "support    54.0  168.000000  0.756757  222.000000    222.000000   \n",
       "\n",
       "           balanced accuracy       auc  \n",
       "precision                0.5  0.613095  \n",
       "recall                   0.5  0.613095  \n",
       "f1-score                 0.5  0.613095  \n",
       "support                  0.5  0.613095  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  54]\n",
      " [  0 168]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the Naive Bayes model\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "\n",
    "# Make predictions\n",
    "pred_prob_nb = model_nb.predict_proba(X_test)[:, 1]\n",
    "pred_label_nb = (pred_prob_nb >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "display(utils.test_report(Y_test=y_test, labels=pred_label_nb, probs=pred_prob_nb))\n",
    "print(confusion_matrix(y_test, pred_label_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.830688</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.748677</td>\n",
       "      <td>0.790791</td>\n",
       "      <td>0.670966</td>\n",
       "      <td>0.816138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.670966</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.670966</td>\n",
       "      <td>0.816138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.879552</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.692649</td>\n",
       "      <td>0.788626</td>\n",
       "      <td>0.670966</td>\n",
       "      <td>0.816138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>0.670966</td>\n",
       "      <td>0.816138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1  accuracy   macro avg  weighted avg  \\\n",
       "precision   0.666667    0.830688  0.806306    0.748677      0.790791   \n",
       "recall      0.407407    0.934524  0.806306    0.670966      0.806306   \n",
       "f1-score    0.505747    0.879552  0.806306    0.692649      0.788626   \n",
       "support    54.000000  168.000000  0.806306  222.000000    222.000000   \n",
       "\n",
       "           balanced accuracy       auc  \n",
       "precision           0.670966  0.816138  \n",
       "recall              0.670966  0.816138  \n",
       "f1-score            0.670966  0.816138  \n",
       "support             0.670966  0.816138  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22  32]\n",
      " [ 11 157]]\n"
     ]
    }
   ],
   "source": [
    "rf_tuning = False\n",
    "\n",
    "if rf_tuning:\n",
    "    # Tuning\n",
    "    model_rf = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 70, 100, 120],\n",
    "        'max_depth': [None, 10, 15, 20, 30],\n",
    "        'min_samples_split': [5, 10, 15, 20, 30],\n",
    "        'min_samples_leaf': [4, 8, 10, 15],\n",
    "        'bootstrap': [True],\n",
    "        'class_weight': ['balanced'],\n",
    "        'criterion': ['log_loss'],  # 'entropy'\n",
    "        'max_features': [None]\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Random search\n",
    "    r_search = RandomizedSearchCV(estimator=model_rf, param_distributions=param_grid, cv=skf, \n",
    "                                n_iter=10, scoring='f1_macro', n_jobs=-1, verbose=1,\n",
    "                                refit=False)\n",
    "\n",
    "    r_search.fit(X_train, y_train)\n",
    "\n",
    "    # Save best parameters\n",
    "    best_params = r_search.best_params_\n",
    "    with open(f'./model/rf/rf_best_params.pickle', 'wb') as handle:\n",
    "        pickle.dump(best_params, handle)\n",
    "else:\n",
    "    with open('./model/rf/rf_best_params.pickle', 'rb') as handle:\n",
    "        best_params = pickle.load(handle)\n",
    "\n",
    "# Random Forest Model\n",
    "model_rf = RandomForestClassifier(**best_params, n_jobs=-1)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "pred_prob_rf = model_rf.predict_proba(X_test)[:, 1]\n",
    "pred_label_rf = (pred_prob_rf >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "display(utils.test_report(Y_test=y_test, labels=pred_label_rf, probs=pred_prob_rf))\n",
    "print(confusion_matrix(y_test, pred_label_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 19682\n",
      "Number of unique POS tags: 1\n",
      "Label distribution: label\n",
      "0    840\n",
      "1    269\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Word vocabulary\n",
    "word_tokens = text['speech_text_word_tokens'].dropna().tolist()\n",
    "all_word_tokens = [token for sublist in word_tokens for token in sublist]\n",
    "word2idx = lm.build_vocab(all_word_tokens, max_vocab_size=50000, min_freq=2)\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# POS vocabulary\n",
    "pos_tags = text['speech_text_pos_tags'].dropna().tolist()\n",
    "all_pos_tags = [tag for sublist in pos_tags for tag in sublist]\n",
    "\n",
    "unique_pos_tags = sorted(set(all_pos_tags))\n",
    "pos_tag2idx = {tag: idx for idx, tag in enumerate(unique_pos_tags, start=1)}\n",
    "pos_tag2idx['<PAD>'] = 0  # Padding token\n",
    "\n",
    "print(f\"Number of unique POS tags: {len(pos_tag2idx)}\")\n",
    "\n",
    "# Encode tokens and POS tags\n",
    "text['encoded_text'] = text['speech_text_word_tokens'].apply(lambda x: lm.encode_text(x, word2idx))\n",
    "text['encoded_pos_tags'] = lm.encode_pos_tags(text['speech_text_pos_tags'].dropna().tolist(), pos_tag2idx)\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {'Positive': 0, 'Negative': 1}\n",
    "text['label'] = list(map(label_mapping.get, text['target_label']))\n",
    "\n",
    "print(f\"Label distribution: {text['label'].value_counts()}\")\n",
    "\n",
    "# Final dataset\n",
    "data = text[['encoded_text', 'encoded_pos_tags', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (17012, 3)\n",
      "Test set size: (222, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train / test set\n",
    "train_df, test_df = data.loc[indices_train], data.loc[indices_test]\n",
    "\n",
    "# Slice train set into windows of fixed length\n",
    "slice_data = True\n",
    "window_size = 150\n",
    "\n",
    "if slice_data:\n",
    "  # Slice train set\n",
    "  sliced_rows = []\n",
    "  for _, row in train_df.iterrows():\n",
    "      sliced_rows.extend(lm.slice_lists(row, window_size))\n",
    "\n",
    "  train_df = pd.DataFrame(sliced_rows, columns=['encoded_text', 'encoded_pos_tags', 'label'])\n",
    "\n",
    "print(f\"Training set size: {train_df.shape}\")\n",
    "print(f\"Test set size: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "train_dataset = lm.SpeechDataset(\n",
    "    train_df['encoded_text'].tolist(),\n",
    "    train_df['encoded_pos_tags'].tolist(),\n",
    "    train_df['label'].tolist()\n",
    ")\n",
    "\n",
    "# valid_dataset = SpeechDataset(\n",
    "#     valid_df['encoded_text'].tolist(),\n",
    "#     valid_df['encoded_pos_tags'].tolist(),\n",
    "#     valid_df['label'].tolist()\n",
    "# )\n",
    "\n",
    "test_dataset = lm.SpeechDataset(\n",
    "    test_df['encoded_text'].tolist(),\n",
    "    test_df['encoded_pos_tags'].tolist(),\n",
    "    test_df['label'].tolist()\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=lm.collate_fn\n",
    ")\n",
    "\n",
    "# valid_loader = DataLoader(\n",
    "#     valid_dataset, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=False, \n",
    "#     collate_fn=collate_fn\n",
    "# )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=lm.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_DIM = 128\n",
    "CELL_STATE_INFO = 'linear'  # 'ignore', 'add', 'linear'\n",
    "OUTPUT_DIM = 1  # Binary Classification\n",
    "N_LAYERS = 2\n",
    "DROPOUT_LSTM = 0.2\n",
    "DROPUT_H_C_STATES = 0.05\n",
    "PAD_IDX = word2idx['<PAD>']\n",
    "POS_PAD_IDX = pos_tag2idx['<PAD>']\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "POS_VOCAB_SIZE = len(pos_tag2idx)\n",
    "EMBEDDING_DIM = 50\n",
    "POS_EMBEDDING_DIM = 32\n",
    "FREEZE_EMBEDDINGS = False\n",
    "POS_EMBEDDINGS = False\n",
    "\n",
    "# Load pretrained embeddings\n",
    "## !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "## !unzip glove.6B.zip\n",
    "\n",
    "# url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "# response = requests.get(url, stream=True)\n",
    "\n",
    "# with open(\"./data/embeddings/glove.6B.zip\", \"wb\") as file:\n",
    "#     for chunk in response.iter_content(chunk_size=1024*1024):\n",
    "#         file.write(chunk)\n",
    "\n",
    "# archive  = zipfile.ZipFile(\"data/embeddings/glove.6B.zip\", \"r\")\n",
    "\n",
    "# for file in archive.namelist():\n",
    "#     if file.startswith('glove.6B.50d.txt'):\n",
    "#         archive.extract(file, 'data/embeddings/')\n",
    "\n",
    "# Create Embedding Matrix\n",
    "embedding_index = {}\n",
    "with open('data/embeddings/glove.6B.50d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coeffs\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():  # iterates through each word in the vocabulary (which is created before)\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Initialize model\n",
    "model_lstm = lm.SentimentLSTM(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    cell_state_info=CELL_STATE_INFO,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout_lstm=DROPOUT_LSTM,\n",
    "    dropout_h_c_states=DROPUT_H_C_STATES,\n",
    "    pad_idx=PAD_IDX,\n",
    "    pos_pad_idx=POS_PAD_IDX,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    pos_vocab_size=POS_VOCAB_SIZE,\n",
    "    pos_embedding_dim=POS_EMBEDDING_DIM,\n",
    "    pretrained_embeddings=embedding_matrix,\n",
    "    freeze_embeddings=FREEZE_EMBEDDINGS,\n",
    "    pos_embeddings=POS_EMBEDDINGS\n",
    ")\n",
    "\n",
    "# Define loss and optimizer\n",
    "target_count = data['label'].value_counts()\n",
    "neg = target_count[0]\n",
    "pos = target_count[1]\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(neg / pos))\n",
    "optimizer = optim.AdamW(model_lstm.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Move model and criterion to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_lstm = model_lstm.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_STATE_INFO = 'linear'\n",
    "DROPUT_H_C_STATES=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = gr.SentimentGRU(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    cell_state_info=CELL_STATE_INFO,  # Add this\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout_gru=DROPOUT_LSTM,  # Use the same dropout for GRU\n",
    "    dropout_h_c_states=DROPUT_H_C_STATES,  # Add this\n",
    "    pad_idx=PAD_IDX,\n",
    "    pos_pad_idx=POS_PAD_IDX,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    pos_vocab_size=POS_VOCAB_SIZE,\n",
    "    pos_embedding_dim=POS_EMBEDDING_DIM,\n",
    "    pretrained_embeddings=embedding_matrix,\n",
    "    freeze_embeddings=FREEZE_EMBEDDINGS,\n",
    "    pos_embeddings=POS_EMBEDDINGS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_gru = model_gru.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "train_dataset = lm.SpeechDataset(\n",
    "    train_df['encoded_text'].tolist(),\n",
    "    train_df['encoded_pos_tags'].tolist(),\n",
    "    train_df['label'].tolist()\n",
    ")\n",
    "\n",
    "# valid_dataset = SpeechDataset(\n",
    "#     valid_df['encoded_text'].tolist(),\n",
    "#     valid_df['encoded_pos_tags'].tolist(),\n",
    "#     valid_df['label'].tolist()\n",
    "# )\n",
    "\n",
    "test_dataset = lm.SpeechDataset(\n",
    "    test_df['encoded_text'].tolist(),\n",
    "    test_df['encoded_pos_tags'].tolist(),\n",
    "    test_df['label'].tolist()\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=lm.collate_fn\n",
    ")\n",
    "\n",
    "# valid_loader = DataLoader(\n",
    "#     valid_dataset, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=False, \n",
    "#     collate_fn=collate_fn\n",
    "# )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=lm.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model_gru.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(neg / pos).to(device))\n",
    "NUM_EPOCHS = 25\n",
    "# # Training loop\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     train_loss, train_metrics = gr.train_model_gru(\n",
    "#         model_gru, train_loader, criterion, optimizer, device\n",
    "#     )\n",
    "#     print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train F1: {train_metrics['f1_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\tTrain Loss: 1.0982 | Test Loss: 1.0137\n",
      "\tTrain F1-Score: 0.2982 | Test F1-Score: 0.5853\n",
      "Epoch 2/25\n",
      "\tTrain Loss: 1.0660 | Test Loss: 0.9724\n",
      "\tTrain F1-Score: 0.4790 | Test F1-Score: 0.5775\n",
      "Epoch 3/25\n",
      "\tTrain Loss: 1.0422 | Test Loss: 0.9577\n",
      "\tTrain F1-Score: 0.5150 | Test F1-Score: 0.5577\n",
      "Epoch 4/25\n",
      "\tTrain Loss: 1.0214 | Test Loss: 1.0368\n",
      "\tTrain F1-Score: 0.5397 | Test F1-Score: 0.5800\n",
      "Epoch 5/25\n",
      "\tTrain Loss: 1.0050 | Test Loss: 0.9390\n",
      "\tTrain F1-Score: 0.5785 | Test F1-Score: 0.5975\n",
      "Epoch 6/25\n",
      "\tTrain Loss: 0.9884 | Test Loss: 1.0955\n",
      "\tTrain F1-Score: 0.5949 | Test F1-Score: 0.6074\n",
      "Epoch 7/25\n",
      "\tTrain Loss: 0.9757 | Test Loss: 0.8919\n",
      "\tTrain F1-Score: 0.6055 | Test F1-Score: 0.6541\n",
      "Epoch 8/25\n",
      "\tTrain Loss: 0.9648 | Test Loss: 0.8939\n",
      "\tTrain F1-Score: 0.6196 | Test F1-Score: 0.6441\n",
      "Epoch 9/25\n",
      "\tTrain Loss: 0.9515 | Test Loss: 0.8933\n",
      "\tTrain F1-Score: 0.6298 | Test F1-Score: 0.6858\n",
      "Epoch 10/25\n",
      "\tTrain Loss: 0.9425 | Test Loss: 0.8552\n",
      "\tTrain F1-Score: 0.6348 | Test F1-Score: 0.6476\n",
      "Epoch 11/25\n",
      "\tTrain Loss: 0.9290 | Test Loss: 0.9421\n",
      "\tTrain F1-Score: 0.6457 | Test F1-Score: 0.6504\n",
      "Epoch 12/25\n",
      "\tTrain Loss: 0.9157 | Test Loss: 0.8399\n",
      "\tTrain F1-Score: 0.6545 | Test F1-Score: 0.6754\n",
      "Epoch 13/25\n",
      "\tTrain Loss: 0.8958 | Test Loss: 0.8535\n",
      "\tTrain F1-Score: 0.6719 | Test F1-Score: 0.6648\n",
      "Epoch 14/25\n",
      "\tTrain Loss: 0.8791 | Test Loss: 0.8719\n",
      "\tTrain F1-Score: 0.6835 | Test F1-Score: 0.7150\n",
      "Epoch 15/25\n",
      "\tTrain Loss: 0.8564 | Test Loss: 0.8175\n",
      "\tTrain F1-Score: 0.6979 | Test F1-Score: 0.6830\n",
      "Epoch 16/25\n",
      "\tTrain Loss: 0.8347 | Test Loss: 0.8678\n",
      "\tTrain F1-Score: 0.7086 | Test F1-Score: 0.6899\n",
      "Epoch 17/25\n",
      "\tTrain Loss: 0.8056 | Test Loss: 0.8313\n",
      "\tTrain F1-Score: 0.7255 | Test F1-Score: 0.6874\n",
      "Epoch 18/25\n",
      "\tTrain Loss: 0.7826 | Test Loss: 0.8892\n",
      "\tTrain F1-Score: 0.7414 | Test F1-Score: 0.7114\n",
      "Epoch 19/25\n",
      "\tTrain Loss: 0.7480 | Test Loss: 0.8939\n",
      "\tTrain F1-Score: 0.7646 | Test F1-Score: 0.7092\n",
      "Epoch 20/25\n",
      "\tTrain Loss: 0.7255 | Test Loss: 0.9394\n",
      "\tTrain F1-Score: 0.7752 | Test F1-Score: 0.7073\n",
      "Epoch 21/25\n",
      "\tTrain Loss: 0.7046 | Test Loss: 0.9087\n",
      "\tTrain F1-Score: 0.7862 | Test F1-Score: 0.6638\n",
      "Epoch 22/25\n",
      "\tTrain Loss: 0.6826 | Test Loss: 1.0153\n",
      "\tTrain F1-Score: 0.7978 | Test F1-Score: 0.7073\n",
      "Epoch 23/25\n",
      "\tTrain Loss: 0.6458 | Test Loss: 1.1300\n",
      "\tTrain F1-Score: 0.8215 | Test F1-Score: 0.6872\n",
      "Epoch 24/25\n",
      "\tTrain Loss: 0.6204 | Test Loss: 0.8979\n",
      "\tTrain F1-Score: 0.8311 | Test F1-Score: 0.7217\n",
      "Epoch 25/25\n",
      "\tTrain Loss: 0.6011 | Test Loss: 1.2193\n",
      "\tTrain F1-Score: 0.8443 | Test F1-Score: 0.6578\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.813131</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.760732</td>\n",
       "      <td>0.787640</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.799824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.799824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.879781</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.657839</td>\n",
       "      <td>0.771810</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.799824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.799824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1  accuracy   macro avg  weighted avg  \\\n",
       "precision   0.708333    0.813131  0.801802    0.760732      0.787640   \n",
       "recall      0.314815    0.958333  0.801802    0.636574      0.801802   \n",
       "f1-score    0.435897    0.879781  0.801802    0.657839      0.771810   \n",
       "support    54.000000  168.000000  0.801802  222.000000    222.000000   \n",
       "\n",
       "           balanced accuracy       auc  \n",
       "precision           0.636574  0.799824  \n",
       "recall              0.636574  0.799824  \n",
       "f1-score            0.636574  0.799824  \n",
       "support             0.636574  0.799824  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 17  37]\n",
      " [  7 161]]\n"
     ]
    }
   ],
   "source": [
    "gru_training = True\n",
    "\n",
    "if gru_training:\n",
    "    NUM_EPOCHS = 25\n",
    "\n",
    "    train_f1 = []\n",
    "    test_f1 = []\n",
    "\n",
    "    # Training Loop Without Early Stopping\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss, train_metrics = gr.train_model_gru(\n",
    "            model_gru,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # Evaluate on Test Set\n",
    "        test_loss, test_metrics, _ = gr.test_model_gru(model_gru, test_loader, criterion, device)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}')\n",
    "        print(f'\\tTrain F1-Score: {train_metrics[\"f1_score\"]:.4f} | Test F1-Score: {test_metrics[\"f1_score\"]:.4f}')\n",
    "\n",
    "        train_f1.append(train_metrics[\"f1_score\"])\n",
    "        test_f1.append(test_metrics[\"f1_score\"])\n",
    "\n",
    "        # Save the model\n",
    "        gr.save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model_gru.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, filename=r'model/gru/best_model.pth')\n",
    "else:\n",
    "    # Load the best model\n",
    "    gr.load_checkpoint(r'model/gru/best_model.pth', model_gru, optimizer, device=device)\n",
    "\n",
    "# Make predictions\n",
    "loss_hru, metrics_hru, preds_gru = gr.test_model_gru(model_gru, test_loader, criterion, device)\n",
    "pred_prob_gru, pred_label_gru, _ = preds_gru\n",
    "\n",
    "# Evaluate the model\n",
    "display(utils.test_report(Y_test=y_test, labels=1 - np.array(pred_label_gru), probs=1 - np.array(pred_prob_gru)))\n",
    "print(confusion_matrix(y_test, 1 - np.array(pred_label_gru)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transformer based Model\n",
    "\n",
    "### Model used\n",
    "This model is a fine-tuned version of distilroberta-base on the financial_phrasebank dataset.  \n",
    "Source: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, ClassLabel, Features, Value\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "from scripts import preprocessing as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Parquet file using PyArrow\n",
    "with open(r'data/output_speech_us_central_bank_v2.parquet', 'rb') as handle:\n",
    "    text = pq.read_table(handle).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = pr.TextPreprocessor(\n",
    "    remove_stopwords=True,\n",
    "    apply_pos_tagging=True,      # Enable POS tagging\n",
    "    apply_lemmatization=True     # Enable lemmatization\n",
    ")\n",
    "\n",
    "text[['speech_text_processed_text', 'speech_text_word_tokens',\n",
    "      'speech_text_sent_tokens', 'speech_text_word_tokens_wo_stopwords',\n",
    "      'speech_text_pos_tags']] = preprocessor.preprocess_dataframe(text, 'speech_text')\n",
    "\n",
    "text.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['processed_speech_text_2'] = text['speech_text_word_tokens_wo_stopwords'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED USAMA\\AppData\\Local\\Temp\\ipykernel_22644\\4086762478.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['target_label'] = label_encoder.fit_transform(data['target_label'])\n"
     ]
    }
   ],
   "source": [
    "data = text[[ 'target_label','speech_text_processed_text']]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "data['target_label'] = label_encoder.fit_transform(data['target_label'])\n",
    "\n",
    "data = data.rename(columns={'speech_text_processed_text':'text','target_label':'label'})\n",
    "\n",
    "y = data['label']\n",
    "X = data['text']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, data.index,\n",
    "                                                                                 stratify=y, test_size=0.2,\n",
    "                                                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want the same train test split for all the models so loading the indices of the train and test sets\n",
    "indices = pd.read_excel(r'data/idx.xlsx')\n",
    "indices_train = indices[0]\n",
    "indices_test = indices[indices[1].notna()][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset  = data.loc[indices_train].copy()\n",
    "val_dataset   = data.loc[indices_test].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1:\n",
    "Since the max token size accepted by the transformer is 512 tokens, we will split the text with higher than 500 tokens into chunks and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, label, chunk_size=500):\n",
    "    words = text.split()  # Split the text into words\n",
    "    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return [{\"text\": chunk, \"label\": label} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row of the DataFrame\n",
    "result = []\n",
    "for _, row in train_dataset.iterrows():\n",
    "    result.extend(split_text_into_chunks(row[\"text\"], row[\"label\"]))\n",
    "\n",
    "# Convert the result into a new DataFrame\n",
    "split_df = pd.DataFrame(result)\n",
    "\n",
    "split_df['word_count'] = split_df['text'].apply(lambda x: len(x.split()))\n",
    "# New train dataset\n",
    "train_dataset = split_df[['label','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5208\n",
      "Validation samples: 222\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "val_dataset = Dataset.from_pandas(val_dataset)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 222\n"
     ]
    }
   ],
   "source": [
    "### Creating copy of val dataset just to check if the loaded models is reproducible\n",
    "test_dataset = data.loc[indices_test].copy()\n",
    "test_dataset = Dataset.from_pandas(test_dataset)\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting up GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bd60c32bc1472dbb9360a19a704c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68043778cded4685809a1b6de89cd482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "model_name = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        #max_length=128  # Adjust based on your data\n",
    "    )\n",
    "\n",
    "# Tokenizing the training set\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Tokenizing the validation set\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model for sequence classification\n",
    "# Since the transformer is intially for 3 labels, we are setting up new labels\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOHAMMED USAMA\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",report_to=\"none\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# import google.generativeai as genai\n",
    "# # Used to securely store your API key\n",
    "# from google.colab import userdata\n",
    "\n",
    "\n",
    "# Hugging_Face_NLP=userdata.get('Hugging_Face_NLP')\n",
    "\n",
    "\n",
    "from scripts import config  #This scripts is for Hugging face API. Please create the relevant API key file\n",
    "\n",
    "# Login using your API key\n",
    "login(token=config.Hugging_Face_NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dce2007c46f4823ad365a87c9d04261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1953 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3682, 'grad_norm': 41.941734313964844, 'learning_rate': 3.719918074756785e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb04c6b4cfa42cdaeac5873a4ecec52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7911430597305298, 'eval_accuracy': 0.7792792792792793, 'eval_runtime': 7.1853, 'eval_samples_per_second': 30.896, 'eval_steps_per_second': 3.897, 'epoch': 1.0}\n",
      "{'loss': 0.3177, 'grad_norm': 51.35104751586914, 'learning_rate': 2.439836149513569e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c210eb920e48399977bb48673ee7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9518962502479553, 'eval_accuracy': 0.7567567567567568, 'eval_runtime': 7.1266, 'eval_samples_per_second': 31.151, 'eval_steps_per_second': 3.929, 'epoch': 2.0}\n",
      "{'loss': 0.2275, 'grad_norm': 92.83016204833984, 'learning_rate': 1.1597542242703534e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce7c5824a924acfab5c516087939a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1411126852035522, 'eval_accuracy': 0.7567567567567568, 'eval_runtime': 7.3337, 'eval_samples_per_second': 30.271, 'eval_steps_per_second': 3.818, 'epoch': 3.0}\n",
      "{'train_runtime': 2187.7731, 'train_samples_per_second': 7.142, 'train_steps_per_second': 0.893, 'train_loss': 0.27990975406801033, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1953, training_loss=0.27990975406801033, metrics={'train_runtime': 2187.7731, 'train_samples_per_second': 7.142, 'train_steps_per_second': 0.893, 'total_flos': 2069670636601344.0, 'train_loss': 0.27990975406801033, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716dbfd358d644a6becaf048c4f685ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "logits = predictions.predictions  # Shape: (num_samples, num_classes)\n",
    "\n",
    "# Apply SoftMax to convert logits to probabilities\n",
    "probabilities = softmax(logits, axis=1)\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "prob_df = pd.DataFrame(probabilities, columns=[0,1])\n",
    "probabilities = prob_df[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.739640</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.765542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.765542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.641164</td>\n",
       "      <td>0.745748</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.765542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.765542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1  accuracy   macro avg  weighted avg  \\\n",
       "precision   0.500000    0.816667  0.756757    0.658333      0.739640   \n",
       "recall      0.388889    0.875000  0.756757    0.631944      0.756757   \n",
       "f1-score    0.437500    0.844828  0.756757    0.641164      0.745748   \n",
       "support    54.000000  168.000000  0.756757  222.000000    222.000000   \n",
       "\n",
       "           balanced accuracy       auc  \n",
       "precision           0.631944  0.765542  \n",
       "recall              0.631944  0.765542  \n",
       "f1-score            0.631944  0.765542  \n",
       "support             0.631944  0.765542  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21  33]\n",
      " [ 21 147]]\n"
     ]
    }
   ],
   "source": [
    "from scripts import utils\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model\n",
    "display(utils.test_report(Y_test=val_dataset['label'], labels=predicted_labels, probs=probabilities))\n",
    "print(confusion_matrix(val_dataset['label'], predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2:\n",
    "Alternatively, if we don't set the max tokens parameters and keep the input data as it is then it truncates the inputs accordlingly and process it for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the train and validation dataset\n",
    "train_dataset  = data.loc[indices_train].copy()\n",
    "val_dataset   = data.loc[indices_test].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bd60c32bc1472dbb9360a19a704c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68043778cded4685809a1b6de89cd482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "model_name = \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        #max_length=128  # Adjust based on your data\n",
    "    )\n",
    "\n",
    "# Tokenizing the training set\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Tokenizing the validation set\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model for sequence classification\n",
    "# Since the transformer is intially for 3 labels, we are setting up new labels\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",report_to=\"none\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from scripts import config\n",
    "\n",
    "# Login using your API key\n",
    "login(token=config.Hugging_Face_NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(val_dataset)\n",
    "logits = predictions.predictions  # Shape: (num_samples, num_classes)\n",
    "\n",
    "# Apply SoftMax to convert logits to probabilities\n",
    "probabilities = softmax(logits, axis=1)\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "prob_df = pd.DataFrame(probabilities, columns=[0,1])\n",
    "probabilities = prob_df[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.802419</td>\n",
       "      <td>0.829337</td>\n",
       "      <td>0.723214</td>\n",
       "      <td>0.876213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.723214</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.723214</td>\n",
       "      <td>0.876213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.749153</td>\n",
       "      <td>0.825744</td>\n",
       "      <td>0.723214</td>\n",
       "      <td>0.876213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>54.00</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>0.723214</td>\n",
       "      <td>0.876213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1  accuracy   macro avg  weighted avg  \\\n",
       "precision   0.75    0.854839  0.837838    0.802419      0.829337   \n",
       "recall      0.50    0.946429  0.837838    0.723214      0.837838   \n",
       "f1-score    0.60    0.898305  0.837838    0.749153      0.825744   \n",
       "support    54.00  168.000000  0.837838  222.000000    222.000000   \n",
       "\n",
       "           balanced accuracy       auc  \n",
       "precision           0.723214  0.876213  \n",
       "recall              0.723214  0.876213  \n",
       "f1-score            0.723214  0.876213  \n",
       "support             0.723214  0.876213  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 27  27]\n",
      " [  9 159]]\n"
     ]
    }
   ],
   "source": [
    "from scripts import utils\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model\n",
    "display(utils.test_report(Y_test=val_dataset['label'], labels=predicted_labels, probs=probabilities))\n",
    "print(confusion_matrix(val_dataset['label'], predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
