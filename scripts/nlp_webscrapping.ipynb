{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c036a016-e1c7-4a4a-b32b-98cbb8c50705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Tuple\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pyarrow import Table\n",
    "from pyarrow.parquet import write_table\n",
    "from requests.exceptions import RequestException, Timeout\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas_datareader.data as web\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1a793-648c-43eb-a476-012514e7b6f9",
   "metadata": {},
   "source": [
    "## 1. Scraping US Central Bank Announcements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621ac333-10ef-44f0-919b-ca3a5e0f8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_metadata(url: str,\n",
    "                        wait_time: int,\n",
    "                        page_limit: int) -> Tuple[List[str], List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Selenium initialization\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open a webpage (necessary to load JavaScript content)\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(wait_time)\n",
    "\n",
    "    # Going to the next page\n",
    "    event_dates, titles, urls, speakers, locations = [], [], [], [], []\n",
    "    page_count = 1\n",
    "\n",
    "    while page_count <= page_limit:\n",
    "        # Get/parse data\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Get lists with tags\n",
    "        events = soup.find_all('div', class_='col-xs-9 col-md-10 eventlist__event')\n",
    "        dates = soup.find_all('div', class_='col-xs-3 col-md-2 eventlist__time')\n",
    "\n",
    "        # Log page processing\n",
    "        print(f'* Processing page # {page_count} / {page_limit}')\n",
    "\n",
    "        # Extract data from tags\n",
    "        for event, date in zip(events, dates):\n",
    "            # Get date\n",
    "            date_tag = date.find('time', class_='itemDate ng-binding')\n",
    "            dmy = date_tag.text.strip() if date_tag else 'N/A'\n",
    "\n",
    "            # Get event metadata\n",
    "            title_tag = event.find('a', class_='ng-binding')\n",
    "            title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "            url = title_tag['href'] if title_tag and 'href' in title_tag.attrs else 'N/A'\n",
    "\n",
    "            speaker_tag = event.find('p', class_='news__speaker ng-binding')\n",
    "            speaker = speaker_tag.text.strip() if speaker_tag else 'N/A'\n",
    "\n",
    "            location_tag = event.find('p', class_='result__location ng-binding')\n",
    "            location = location_tag.text.strip() if location_tag else 'N/A'\n",
    "\n",
    "            # Append data tags\n",
    "            event_dates.append(dmy)\n",
    "            titles.append(title)\n",
    "            urls.append(url)\n",
    "            speakers.append(speaker)\n",
    "            locations.append(location)\n",
    "\n",
    "        # Go to the next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.LINK_TEXT, 'Next')\n",
    "            driver.execute_script('arguments[0].click();', next_button)  # next_button.click()\n",
    "            time.sleep(2)\n",
    "            page_count += 1\n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            print('No 'Next' button was found')\n",
    "\n",
    "        # Log item processing per page\n",
    "        print(f'    * Processed {len(titles)} items...')\n",
    "\n",
    "    # Close the page\n",
    "    driver.quit()\n",
    "\n",
    "    return event_dates, titles, urls, speakers, locations\n",
    "\n",
    "\n",
    "def get_speech_text(url_list: list, wait_time: int) -> list:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    speech_content = []\n",
    "\n",
    "    for idx, url in enumerate(url_list):\n",
    "        try:\n",
    "            # Get data\n",
    "            url = f'https://www.federalreserve.gov/{url}'\n",
    "            page = requests.get(url, timeout=wait_time)\n",
    "\n",
    "            # Validate the request\n",
    "            page.raise_for_status()\n",
    "\n",
    "            # Parse data\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            # Get speech text\n",
    "            speech = soup.find_all('div', {'class': 'col-xs-12 col-sm-8 col-md-8'})[0].text\n",
    "            speech_content.append(speech)\n",
    "\n",
    "            # Log speech processing\n",
    "            if idx % 100 == 0:\n",
    "                print(f'* Processing speech # {idx} / {len(url_list)}')\n",
    "        except Timeout:\n",
    "            print('The request timed out')\n",
    "        except RequestException as e:\n",
    "            print(f'An error occurred: {e}')\n",
    "        except ValueError as e:\n",
    "            print(f'Content error: {e}')\n",
    "\n",
    "    return speech_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870d076b-0a8a-462d-9d54-db75fd5cff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Processing page # 1 / 57\n",
      "    * Processed 0 items...\n",
      "* Processing page # 2 / 57\n",
      "    * Processed 20 items...\n",
      "* Processing page # 3 / 57\n",
      "    * Processed 40 items...\n",
      "* Processing page # 4 / 57\n",
      "    * Processed 60 items...\n",
      "* Processing page # 5 / 57\n",
      "    * Processed 80 items...\n",
      "* Processing page # 6 / 57\n",
      "    * Processed 100 items...\n",
      "* Processing page # 7 / 57\n",
      "    * Processed 120 items...\n",
      "* Processing page # 8 / 57\n",
      "    * Processed 140 items...\n",
      "* Processing page # 9 / 57\n",
      "    * Processed 160 items...\n",
      "* Processing page # 10 / 57\n",
      "    * Processed 180 items...\n",
      "* Processing page # 11 / 57\n",
      "    * Processed 200 items...\n",
      "* Processing page # 12 / 57\n",
      "    * Processed 220 items...\n",
      "* Processing page # 13 / 57\n",
      "    * Processed 240 items...\n",
      "* Processing page # 14 / 57\n",
      "    * Processed 260 items...\n",
      "* Processing page # 15 / 57\n",
      "    * Processed 280 items...\n",
      "* Processing page # 16 / 57\n",
      "    * Processed 300 items...\n",
      "* Processing page # 17 / 57\n",
      "    * Processed 320 items...\n",
      "* Processing page # 18 / 57\n",
      "    * Processed 340 items...\n",
      "* Processing page # 19 / 57\n",
      "    * Processed 360 items...\n",
      "* Processing page # 20 / 57\n",
      "    * Processed 380 items...\n",
      "* Processing page # 21 / 57\n",
      "    * Processed 400 items...\n",
      "* Processing page # 22 / 57\n",
      "    * Processed 420 items...\n",
      "* Processing page # 23 / 57\n",
      "    * Processed 440 items...\n",
      "* Processing page # 24 / 57\n",
      "    * Processed 460 items...\n",
      "* Processing page # 25 / 57\n",
      "    * Processed 480 items...\n",
      "* Processing page # 26 / 57\n",
      "    * Processed 500 items...\n",
      "* Processing page # 27 / 57\n",
      "    * Processed 520 items...\n",
      "* Processing page # 28 / 57\n",
      "    * Processed 540 items...\n",
      "* Processing page # 29 / 57\n",
      "    * Processed 560 items...\n",
      "* Processing page # 30 / 57\n",
      "    * Processed 580 items...\n",
      "* Processing page # 31 / 57\n",
      "    * Processed 600 items...\n",
      "* Processing page # 32 / 57\n",
      "    * Processed 620 items...\n",
      "* Processing page # 33 / 57\n",
      "    * Processed 640 items...\n",
      "* Processing page # 34 / 57\n",
      "    * Processed 660 items...\n",
      "* Processing page # 35 / 57\n",
      "    * Processed 680 items...\n",
      "* Processing page # 36 / 57\n",
      "    * Processed 700 items...\n",
      "* Processing page # 37 / 57\n",
      "    * Processed 720 items...\n",
      "* Processing page # 38 / 57\n",
      "    * Processed 740 items...\n",
      "* Processing page # 39 / 57\n",
      "    * Processed 760 items...\n",
      "* Processing page # 40 / 57\n",
      "    * Processed 780 items...\n",
      "* Processing page # 41 / 57\n",
      "    * Processed 800 items...\n",
      "* Processing page # 42 / 57\n",
      "    * Processed 820 items...\n",
      "* Processing page # 43 / 57\n",
      "    * Processed 840 items...\n",
      "* Processing page # 44 / 57\n",
      "    * Processed 860 items...\n",
      "* Processing page # 45 / 57\n",
      "    * Processed 880 items...\n",
      "* Processing page # 46 / 57\n",
      "    * Processed 900 items...\n",
      "* Processing page # 47 / 57\n",
      "    * Processed 920 items...\n",
      "* Processing page # 48 / 57\n",
      "    * Processed 940 items...\n",
      "* Processing page # 49 / 57\n",
      "    * Processed 960 items...\n",
      "* Processing page # 50 / 57\n",
      "    * Processed 980 items...\n",
      "* Processing page # 51 / 57\n",
      "    * Processed 1000 items...\n",
      "* Processing page # 52 / 57\n",
      "    * Processed 1020 items...\n",
      "* Processing page # 53 / 57\n",
      "    * Processed 1040 items...\n",
      "* Processing page # 54 / 57\n",
      "    * Processed 1060 items...\n",
      "* Processing page # 55 / 57\n",
      "    * Processed 1080 items...\n",
      "* Processing page # 56 / 57\n",
      "    * Processed 1100 items...\n",
      "* Processing page # 57 / 57\n",
      "    * Processed 1113 items...\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.federalreserve.gov/newsevents/speeches.htm'\n",
    "speech_dates, speech_titles, speech_urls, speech_speakers, _ = get_speech_metadata(url=url, wait_time=10, page_limit=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c41716-703b-4b7f-8f19-202c7ac3b36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Processing speech # 0 / 1113\n",
      "* Processing speech # 100 / 1113\n",
      "* Processing speech # 200 / 1113\n",
      "* Processing speech # 300 / 1113\n",
      "* Processing speech # 400 / 1113\n",
      "* Processing speech # 500 / 1113\n",
      "* Processing speech # 600 / 1113\n",
      "* Processing speech # 700 / 1113\n",
      "* Processing speech # 800 / 1113\n",
      "* Processing speech # 900 / 1113\n",
      "* Processing speech # 1000 / 1113\n",
      "* Processing speech # 1100 / 1113\n"
     ]
    }
   ],
   "source": [
    "speech_text = get_speech_text(url_list=speech_urls, wait_time=10)\n",
    "\n",
    "speech_us_central_bank = {\n",
    "    'speech_date': speech_dates,\n",
    "    'speech_title': speech_titles,\n",
    "    'speech_url': speech_urls,\n",
    "    'speech_speaker': speech_speakers,\n",
    "    'speech_text': speech_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff8ea7-a5a3-4f18-b7cb-a7cf2f6abfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\\\data\\\\speech_us_central_bank.parquet', 'wb') as handle:\n",
    "    write_table(Table.from_pandas(pd.DataFrame(speech_us_central_bank)), handle, compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82d5ec-aff1-4b61-a52e-e38652da039e",
   "metadata": {},
   "source": [
    "## 2. Macroeconomic Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a40aee8e-d083-49f3-a940-8c7f7d5ca299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\yfinance\\utils.py:771: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\yfinance\\utils.py:771: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\yfinance\\utils.py:771: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\yfinance\\utils.py:771: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\yfinance\\utils.py:771: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\yfinance\\utils.py:771: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_projects\\0_venvs\\venv_base\\Lib\\site-packages\\yfinance\\utils.py:771: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "C:\\conda_tmp\\ipykernel_10044\\2283916972.py:48: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data = merged_data.fillna(method='ffill').dropna().reset_index().rename(columns={'index': 'date'})\n",
      "C:\\conda_tmp\\ipykernel_10044\\2283916972.py:58: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Date range\n",
    "start_date = datetime(2006, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "# 1. Macroeconomic Activity and Growth\n",
    "gdp_data = web.DataReader('GDP', 'fred', start_date, end_date)  # US GDP (quarterly)\n",
    "unemployment_data = web.DataReader('UNRATE', 'fred', start_date, end_date)  # US unemployment rate (monthly)\n",
    "\n",
    "# 2. Inflation and Price Levels\n",
    "cpi_data = web.DataReader('CPIAUCSL', 'fred', start_date, end_date)  # US Consumer Price Index (CPI), i.e. a measure of inflation (monthly)\n",
    "\n",
    "# 3. Monetary Policy and Interest Rates\n",
    "treasury_yield_data = yf.download('^TNX', start=start_date, end=end_date, interval='1d')[['Close']].rename(columns={'Close': '10Y_Treasury_Yield'})  # US 10-Year Treasury Yield, i.e. debt obligations (daily)\n",
    "fed_funds_rate = web.DataReader('FEDFUNDS', 'fred', start_date, end_date)  # Federal Funds Rate (daily)\n",
    "\n",
    "# 4. Market Sentiment and Risk\n",
    "vix_data = yf.download('^VIX', start=start_date, end=end_date, interval='1d')[['Close']].rename(columns={'Close': 'VIX'})  # US Volatility Index, i.e. 'Fear Gauge' (daily)\n",
    "consumer_sentiment = web.DataReader('UMCSENT', 'fred', start_date, end_date)  # Consumer Sentiment (monthly)\n",
    "\n",
    "# 5. Currency and Exchange Rate\n",
    "dxy_data = yf.download('DX-Y.NYB', start=start_date, end=end_date, interval='1d')[['Close']].rename(columns={'Close': 'DXY'})  # US Dollar Index (DXY) (daily)\n",
    "\n",
    "# 6. Housing Market Data\n",
    "home_price_index = web.DataReader('CSUSHPINSA', 'fred', start_date, end_date)  # US Housing Price Index (monthly)\n",
    "\n",
    "# Commodities Prices (daily)\n",
    "oil_data = yf.download('CL=F', start=start_date, end=end_date, interval='1d')[['Close']].rename(columns={'Close': 'Oil'})\n",
    "gold_data = yf.download('GC=F', start=start_date, end=end_date, interval='1d')[['Close']].rename(columns={'Close': 'Gold'})\n",
    "natural_gas_data = yf.download('NG=F', start=start_date, end=end_date, interval='1d')[['Close']].rename(columns={'Close': 'Natural_Gas'})\n",
    "copper_data = yf.download('HG=F', start=start_date, end=end_date, interval='1d')[['Close']].rename(columns={'Close': 'Copper'})\n",
    "\n",
    "# Merge all datasets\n",
    "merged_data = (treasury_yield_data \\\n",
    "    .merge(gdp_data, how='outer', left_index=True, right_index=True)\n",
    "    .merge(unemployment_data, how='outer', left_index=True, right_index=True)\n",
    "    .merge(cpi_data, how='outer', left_index=True, right_index=True)\n",
    "    .merge(fed_funds_rate, how='outer', left_index=True, right_index=True)\n",
    "    .merge(vix_data, how='outer', left_index=True, right_index=True)\n",
    "    .merge(consumer_sentiment, how='outer', left_index=True, right_index=True)\n",
    "    .merge(dxy_data, how='outer', left_index=True, right_index=True)\n",
    "    .merge(home_price_index , how='outer', left_index=True, right_index=True)\n",
    "    .merge(oil_data , how='outer', left_index=True, right_index=True)\n",
    "    .merge(gold_data , how='outer', left_index=True, right_index=True)\n",
    "    .merge(natural_gas_data , how='outer', left_index=True, right_index=True)\n",
    "    .merge(copper_data , how='outer', left_index=True, right_index=True))\n",
    "\n",
    "# Forward-filling NaNs\n",
    "merged_data = merged_data.fillna(method='ffill').dropna().reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "# Recover weekends\n",
    "date_range = pd.date_range(start=merged_data['date'].min(), end=merged_data['date'].max())\n",
    "merged_data.set_index('date', inplace=True)\n",
    "merged_data = merged_data.reindex(date_range)\n",
    "merged_data.index.name = 'date'\n",
    "merged_data.reset_index(inplace=True)\n",
    "\n",
    "# Fill missing data\n",
    "merged_data.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1814caab-a94b-40f0-8864-6e2d0af1da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\\\data\\\\us_macroeconomic_indicators.parquet', 'wb') as handle:\n",
    "    write_table(Table.from_pandas(merged_data), handle, compression='GZIP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
